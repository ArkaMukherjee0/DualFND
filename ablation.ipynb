{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get('API_KEY')\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import concurrent.futures\n",
    "import re\n",
    "import threading\n",
    "from functools import lru_cache\n",
    "\n",
    "# Regular expression to extract assessment value\n",
    "ASSESSMENT_PATTERN = re.compile(r'<overall_assessment>([01]|REAL|FAKE)</overall_assessment>')\n",
    "\n",
    "def single_model_detection(client, news_text, model_name):\n",
    "    \"\"\"\n",
    "    Test a single model's performance on fake news detection\n",
    "    \n",
    "    Args:\n",
    "        client: The Gemini API client\n",
    "        news_text: The news text to analyze\n",
    "        model_name: Either 'gemini-1.5-pro' or 'gemini-2.0-flash'\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with verification result and processing time\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze this news content and determine if it's real (1) or fake (0). \n",
    "    \n",
    "    News content: {news_text}\n",
    "    \n",
    "    Respond using exactly this format:\n",
    "    <verification>\n",
    "      <overall_assessment>1 for REAL or 0 for FAKE</overall_assessment>\n",
    "      <confidence_score>0-1</confidence_score>\n",
    "      <key_issues>Brief issues</key_issues>\n",
    "      <reasoning>Brief reasoning</reasoning>\n",
    "    </verification>\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model_name,\n",
    "            contents=prompt\n",
    "        )\n",
    "        result = response.text\n",
    "        \n",
    "        # Ensure it has the required XML format\n",
    "        if not \"<verification>\" in result:\n",
    "            result = f\"\"\"<verification>\n",
    "  <overall_assessment>0</overall_assessment>\n",
    "  <confidence_score>0.5</confidence_score>\n",
    "  <key_issues>Malformed response</key_issues>\n",
    "  <reasoning>Could not properly analyze the content</reasoning>\n",
    "</verification>\"\"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name}: {e}\")\n",
    "        result = f\"\"\"<verification>\n",
    "  <overall_assessment>0</overall_assessment>\n",
    "  <confidence_score>0.0</confidence_score>\n",
    "  <key_issues>API error</key_issues>\n",
    "  <reasoning>{model_name} encountered an error</reasoning>\n",
    "</verification>\"\"\"\n",
    "        \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        \"verification\": result,\n",
    "        \"processing_time\": processing_time\n",
    "    }\n",
    "\n",
    "def two_agent_detection(client, news_text):\n",
    "    \"\"\"\n",
    "    Our original 2-agent framework\n",
    "    \n",
    "    Args:\n",
    "        client: The Gemini API client\n",
    "        news_text: The news text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with facts, verification result and processing time\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Use Agent 1 (Gemini 1.5 Pro) to extract facts\n",
    "    facts_prompt = f\"\"\"\n",
    "    Extract verifiable claims from this text as CSV with headers \"claim,source,confidence\":\n",
    "    {news_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response1 = client.models.generate_content(\n",
    "            model='gemini-1.5-pro',\n",
    "            contents=facts_prompt\n",
    "        )\n",
    "        facts_csv = response1.text\n",
    "        \n",
    "        # Ensure it's valid CSV format\n",
    "        if not \"claim,source,confidence\" in facts_csv:\n",
    "            facts_csv = \"claim,source,confidence\\n\" + facts_csv.replace('\\n', ' ').strip()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Agent 1: {e}\")\n",
    "        facts_csv = \"claim,source,confidence\\nError processing input,agent1,0.0\"\n",
    "    \n",
    "    # Step 2: Use Agent 2 (Gemini 2.0 Flash) to verify using the facts\n",
    "    # Create a more compact facts representation\n",
    "    try:\n",
    "        facts_formatted = facts_csv.replace(\"\\n\", \"; \")\n",
    "    except:\n",
    "        facts_formatted = facts_csv\n",
    "    \n",
    "    verify_prompt = f\"\"\"\n",
    "    Classify if this news is real (1) or fake (0) based on these facts. Return in XML:\n",
    "    <verification>\n",
    "      <overall_assessment>1 for REAL or 0 for FAKE</overall_assessment>\n",
    "      <confidence_score>0-1</confidence_score>\n",
    "      <key_issues>Brief issues</key_issues>\n",
    "      <reasoning>Brief reasoning</reasoning>\n",
    "    </verification>\n",
    "    \n",
    "    News: {news_text}\n",
    "    Facts: {facts_formatted}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response2 = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=verify_prompt\n",
    "        )\n",
    "        verification = response2.text\n",
    "        \n",
    "        # Ensure it has the required XML format\n",
    "        if not \"<verification>\" in verification:\n",
    "            verification = f\"\"\"<verification>\n",
    "  <overall_assessment>0</overall_assessment>\n",
    "  <confidence_score>0.5</confidence_score>\n",
    "  <key_issues>Malformed response</key_issues>\n",
    "  <reasoning>Could not properly analyze the content</reasoning>\n",
    "</verification>\"\"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Agent 2: {e}\")\n",
    "        verification = \"\"\"<verification>\n",
    "  <overall_assessment>0</overall_assessment>\n",
    "  <confidence_score>0.0</confidence_score>\n",
    "  <key_issues>Error processing input</key_issues>\n",
    "  <reasoning>Agent 2 encountered an error</reasoning>\n",
    "</verification>\"\"\"\n",
    "        \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "        \n",
    "    return {\n",
    "        \"facts_extracted\": facts_csv,\n",
    "        \"verification\": verification,\n",
    "        \"processing_time\": processing_time\n",
    "    }\n",
    "\n",
    "def extract_assessment(verification_xml):\n",
    "    \"\"\"\n",
    "    Extract the overall assessment (1 or 0) from the verification XML\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regex pattern for faster extraction\n",
    "        match = ASSESSMENT_PATTERN.search(verification_xml)\n",
    "        if match:\n",
    "            assessment_str = match.group(1)\n",
    "            if assessment_str == \"1\" or assessment_str.upper() == \"REAL\":\n",
    "                return 1\n",
    "            elif assessment_str == \"0\" or assessment_str.upper() == \"FAKE\":\n",
    "                return 0\n",
    "        \n",
    "        # Fallback to simple string check\n",
    "        if \"<overall_assessment>1</overall_assessment>\" in verification_xml:\n",
    "            return 1\n",
    "        elif \"<overall_assessment>0</overall_assessment>\" in verification_xml:\n",
    "            return 0\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting assessment: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_dataset_line(line):\n",
    "    \"\"\"\n",
    "    Parse a line from the Uni-Fakkedit-55k dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract text\n",
    "        text_parts = line.split(\"[TEXT]\")\n",
    "        if len(text_parts) != 2:\n",
    "            return None\n",
    "        after_text = text_parts[1]\n",
    "\n",
    "        # Extract objects\n",
    "        text_objects_parts = after_text.split(\"[OBJECTS]\")\n",
    "        if len(text_objects_parts) != 2:\n",
    "            return None\n",
    "        text_part = text_objects_parts[0].strip()\n",
    "        after_objects = text_objects_parts[1]\n",
    "\n",
    "        # Extract label\n",
    "        objects_label_parts = after_objects.split(\"[LABEL]\")\n",
    "        if len(objects_label_parts) != 2:\n",
    "            return None\n",
    "        objects_part = objects_label_parts[0].strip()\n",
    "        label_part = objects_label_parts[1].strip()\n",
    "\n",
    "        # Combine text and objects - keep it short\n",
    "        combined_input = f\"{text_part}. Objects: {objects_part}\"\n",
    "        label_int = int(label_part)\n",
    "        \n",
    "        return (combined_input, label_int)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing line: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_entry(client, line):\n",
    "    \"\"\"\n",
    "    Process a single dataset entry with all three approaches\n",
    "    \"\"\"\n",
    "    parsed_data = parse_dataset_line(line)\n",
    "    if parsed_data is None:\n",
    "        return None\n",
    "    \n",
    "    combined_input, true_label = parsed_data\n",
    "    \n",
    "    # Test with Gemini 1.5 Pro\n",
    "    pro_result = single_model_detection(client, combined_input, 'gemini-1.5-pro')\n",
    "    pro_prediction = extract_assessment(pro_result[\"verification\"])\n",
    "    pro_correct = (pro_prediction == true_label) if pro_prediction is not None else False\n",
    "    \n",
    "    # Test with Gemini 2.0 Flash\n",
    "    flash_result = single_model_detection(client, combined_input, 'gemini-2.0-flash')\n",
    "    flash_prediction = extract_assessment(flash_result[\"verification\"])\n",
    "    flash_correct = (flash_prediction == true_label) if flash_prediction is not None else False\n",
    "    \n",
    "    # Test with two-agent approach\n",
    "    two_agent_result = two_agent_detection(client, combined_input)\n",
    "    two_agent_prediction = extract_assessment(two_agent_result[\"verification\"])\n",
    "    two_agent_correct = (two_agent_prediction == true_label) if two_agent_prediction is not None else False\n",
    "    \n",
    "    return {\n",
    "        'text': combined_input,\n",
    "        'true_label': true_label,\n",
    "        \n",
    "        'pro_prediction': pro_prediction,\n",
    "        'pro_correct': pro_correct,\n",
    "        'pro_verification': pro_result[\"verification\"],\n",
    "        'pro_time': pro_result[\"processing_time\"],\n",
    "        \n",
    "        'flash_prediction': flash_prediction,\n",
    "        'flash_correct': flash_correct,\n",
    "        'flash_verification': flash_result[\"verification\"],\n",
    "        'flash_time': flash_result[\"processing_time\"],\n",
    "        \n",
    "        'two_agent_prediction': two_agent_prediction,\n",
    "        'two_agent_correct': two_agent_correct,\n",
    "        'two_agent_facts': two_agent_result[\"facts_extracted\"],\n",
    "        'two_agent_verification': two_agent_result[\"verification\"],\n",
    "        'two_agent_time': two_agent_result[\"processing_time\"]\n",
    "    }\n",
    "\n",
    "def process_batch(client, batch_lines, lock, writer=None):\n",
    "    \"\"\"\n",
    "    Process a batch of dataset lines\n",
    "    \"\"\"\n",
    "    batch_results = []\n",
    "    \n",
    "    for line in batch_lines:\n",
    "        result = process_entry(client, line)\n",
    "        if result is not None:\n",
    "            batch_results.append(result)\n",
    "            \n",
    "            # Write result to CSV if provided with a writer\n",
    "            if writer:\n",
    "                with lock:\n",
    "                    writer.writerow(result)\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "def run_ablation_study(client, dataset_path, output_path, num_samples=500, batch_size=5, max_workers=2):\n",
    "    \"\"\"\n",
    "    Run an ablation study comparing Gemini 1.5 Pro, Gemini 2.0 Flash, and 2-Agent approach\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Read the dataset lines\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        lines = []\n",
    "        for _ in range(num_samples):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append(line)\n",
    "    \n",
    "    # Create CSV file and writer\n",
    "    csvfile = open(output_path, 'w', newline='')\n",
    "    fieldnames = [\n",
    "        'text', 'true_label', \n",
    "        'pro_prediction', 'pro_correct', 'pro_verification', 'pro_time',\n",
    "        'flash_prediction', 'flash_correct', 'flash_verification', 'flash_time',\n",
    "        'two_agent_prediction', 'two_agent_correct', 'two_agent_facts', \n",
    "        'two_agent_verification', 'two_agent_time'\n",
    "    ]\n",
    "    fieldnames = [\n",
    "        'true_label', 'two_agent_prediction', 'two_agent_correct', 'two_agent_time'\n",
    "    ]\n",
    "\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Create a thread lock for CSV writing\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    # Process batches with ThreadPoolExecutor for parallel processing\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit batch processing tasks\n",
    "        futures = []\n",
    "        for i in range(0, len(lines), batch_size):\n",
    "            batch_lines = lines[i:i+batch_size]\n",
    "            futures.append(executor.submit(process_batch, client, batch_lines, lock, writer))\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n",
    "            batch_results = future.result()\n",
    "            results.extend(batch_results)\n",
    "    \n",
    "    # Close CSV file\n",
    "    csvfile.close()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_processed = len(results)\n",
    "    \n",
    "    pro_correct = sum(1 for r in results if r['pro_correct'])\n",
    "    pro_accuracy = pro_correct / total_processed if total_processed > 0 else 0\n",
    "    pro_avg_time = sum(r['pro_time'] for r in results) / total_processed if total_processed > 0 else 0\n",
    "    \n",
    "    flash_correct = sum(1 for r in results if r['flash_correct'])\n",
    "    flash_accuracy = flash_correct / total_processed if total_processed > 0 else 0\n",
    "    flash_avg_time = sum(r['flash_time'] for r in results) / total_processed if total_processed > 0 else 0\n",
    "    \n",
    "    two_agent_correct = sum(1 for r in results if r['two_agent_correct'])\n",
    "    two_agent_accuracy = two_agent_correct / total_processed if total_processed > 0 else 0\n",
    "    two_agent_avg_time = sum(r['two_agent_time'] for r in results) / total_processed if total_processed > 0 else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n=== ABLATION STUDY RESULTS ===\")\n",
    "    print(f\"Total samples processed: {total_processed}\")\n",
    "    \n",
    "    print(f\"\\n1. GEMINI 1.5 PRO\")\n",
    "    print(f\"   Accuracy: {pro_accuracy:.4f} ({pro_correct}/{total_processed})\")\n",
    "    print(f\"   Avg processing time: {pro_avg_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\n2. GEMINI 2.0 FLASH\")\n",
    "    print(f\"   Accuracy: {flash_accuracy:.4f} ({flash_correct}/{total_processed})\")\n",
    "    print(f\"   Avg processing time: {flash_avg_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\n3. TWO-AGENT APPROACH\")\n",
    "    print(f\"   Accuracy: {two_agent_accuracy:.4f} ({two_agent_correct}/{total_processed})\")\n",
    "    print(f\"   Avg processing time: {two_agent_avg_time:.2f}s\")\n",
    "    \n",
    "    # Calculate performance improvement\n",
    "    best_single_accuracy = max(pro_accuracy, flash_accuracy)\n",
    "    best_single_model = \"Gemini 1.5 Pro\" if pro_accuracy > flash_accuracy else \"Gemini 2.0 Flash\"\n",
    "    \n",
    "    accuracy_improvement = two_agent_accuracy - best_single_accuracy\n",
    "    relative_improvement = (accuracy_improvement / best_single_accuracy) * 100 if best_single_accuracy > 0 else 0\n",
    "    \n",
    "    print(f\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "    print(f\"Best single model: {best_single_model} ({best_single_accuracy:.4f})\")\n",
    "    print(f\"Two-agent approach: {two_agent_accuracy:.4f}\")\n",
    "    print(f\"Absolute improvement: {accuracy_improvement:.4f}\")\n",
    "    print(f\"Relative improvement: {relative_improvement:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_path}\")\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'pro_accuracy': pro_accuracy,\n",
    "        'pro_avg_time': pro_avg_time,\n",
    "        'flash_accuracy': flash_accuracy,\n",
    "        'flash_avg_time': flash_avg_time,\n",
    "        'two_agent_accuracy': two_agent_accuracy,\n",
    "        'two_agent_avg_time': two_agent_avg_time,\n",
    "        'accuracy_improvement': accuracy_improvement,\n",
    "        'relative_improvement': relative_improvement\n",
    "    }\n",
    "\n",
    "def main(client, dataset_path, output_path=\"results/ablation_study_results.csv\", num_samples=500):\n",
    "    \"\"\"\n",
    "    Main function to run the ablation study\n",
    "    \"\"\"\n",
    "    print(f\"Starting ablation study with {num_samples} samples from {dataset_path}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run ablation study\n",
    "    metrics = run_ablation_study(client, dataset_path, output_path, num_samples)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nAblation study completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "dataset_path = r\"C:\\Users\\CoolA\\Code\\dataset_2way_output.txt\"\n",
    "\n",
    "main(client, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting study with 500 samples from C:\\Users\\CoolA\\Code\\dataset_2way_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 100/100 [26:05<00:00, 15.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STUDY RESULTS ===\n",
      "Total samples processed: 500\n",
      "\n",
      "TWO-AGENT APPROACH\n",
      "   Accuracy: 0.7180 (359/500)\n",
      "   Avg processing time: 6.23s\n",
      "\n",
      "Results saved to results/3_piece_study_results.csv\n",
      "\n",
      "Study completed in 1565.97 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import concurrent.futures\n",
    "import re\n",
    "import threading\n",
    "from functools import lru_cache\n",
    "\n",
    "# Regular expression to extract assessment value\n",
    "ASSESSMENT_PATTERN = re.compile(r'<overall_assessment>([01]|REAL|FAKE)</overall_assessment>')\n",
    "\n",
    "def two_agent_detection(client, news_text):\n",
    "    \"\"\"\n",
    "    Our original 2-agent framework\n",
    "    \n",
    "    Args:\n",
    "        client: The Gemini API client\n",
    "        news_text: The news text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with facts, verification result and processing time\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Use Agent 1 (Gemini 1.5 Pro) to extract facts\n",
    "    facts_prompt = f\"\"\"\n",
    "    You are a fact-checking assistant. Given a news piece, extract a list of factual claims that can be verified.\n",
    "    For each claim, provide:\n",
    "    1. The claim\n",
    "    2. The source of the claim (if mentioned in the text)\n",
    "    3. A confidence score (0-1) on how verifiable this claim is based on specificity\n",
    "    \n",
    "    Format your output as CSV with headers: claim,source,confidence\n",
    "    Do not include any other text, explanations, or formatting - ONLY the CSV data.\n",
    "    \n",
    "    News piece to analyze:\n",
    "    {news_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response1 = client.models.generate_content(\n",
    "            model='gemini-1.5-pro',\n",
    "            contents=facts_prompt\n",
    "        )\n",
    "        facts_csv = response1.text\n",
    "        \n",
    "        # Ensure it's valid CSV format\n",
    "        if not \"claim,source,confidence\" in facts_csv:\n",
    "            facts_csv = \"claim,source,confidence\\n\" + facts_csv.replace('\\n', ' ').strip()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Agent 1: {e}\")\n",
    "        facts_csv = \"claim,source,confidence\\nError processing input,agent1,0.0\"\n",
    "    \n",
    "    # Step 2: Use Agent 2 (Gemini 2.0 Flash) to verify using the facts\n",
    "    # Create a more compact facts representation\n",
    "    try:\n",
    "        facts_formatted = facts_csv.replace(\"\\n\", \"; \")\n",
    "    except:\n",
    "        facts_formatted = facts_csv\n",
    "    \n",
    "    verify_prompt = f\"\"\"\n",
    "    You are a news verification assistant. Analyze the provided news piece and fact-check results to determine if the news is likely fake or genuine.\n",
    "    \n",
    "    News piece:\n",
    "    {news_text}\n",
    "    \n",
    "    Extracted facts and claims:\n",
    "    {facts_formatted}\n",
    "    \n",
    "    Analyze the news piece with the following considerations:\n",
    "    1. Do the extracted facts align with established knowledge?\n",
    "    2. Are there logical inconsistencies in the text?\n",
    "    3. Is the source reliable based on the extracted information?\n",
    "    4. Does the writing use manipulative or emotional language?\n",
    "    5. Are there any verifiable false claims?\n",
    "    \n",
    "    Provide your analysis in the following XML format, with no additional text before or after:\n",
    "    <verification>\n",
    "      <overall_assessment>1 for REAL or 0 for FAKE</overall_assessment>\n",
    "      <confidence_score>0-1 score</confidence_score>\n",
    "      <key_issues>List up to 3 key issues or red flags if any</key_issues>\n",
    "      <reasoning>Brief explanation of your reasoning</reasoning>\n",
    "    </verification>\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response2 = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=verify_prompt\n",
    "        )\n",
    "        verification = response2.text\n",
    "        \n",
    "        # Ensure it has the required XML format\n",
    "        if not \"<verification>\" in verification:\n",
    "            verification = f\"\"\"<verification>\n",
    "  <overall_assessment>0</overall_assessment>\n",
    "  <confidence_score>0.5</confidence_score>\n",
    "  <key_issues>Malformed response</key_issues>\n",
    "  <reasoning>Could not properly analyze the content</reasoning>\n",
    "</verification>\"\"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Agent 2: {e}\")\n",
    "        verification = \"\"\"<verification>\n",
    "  <overall_assessment>0</overall_assessment>\n",
    "  <confidence_score>0.0</confidence_score>\n",
    "  <key_issues>Error processing input</key_issues>\n",
    "  <reasoning>Agent 2 encountered an error</reasoning>\n",
    "</verification>\"\"\"\n",
    "        \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "        \n",
    "    return {\n",
    "        \"facts_extracted\": facts_csv,\n",
    "        \"verification\": verification,\n",
    "        \"processing_time\": processing_time\n",
    "    }\n",
    "\n",
    "def extract_assessment(verification_xml):\n",
    "    \"\"\"\n",
    "    Extract the overall assessment (1 or 0) from the verification XML\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regex pattern for faster extraction\n",
    "        match = ASSESSMENT_PATTERN.search(verification_xml)\n",
    "        if match:\n",
    "            assessment_str = match.group(1)\n",
    "            if assessment_str == \"1\" or assessment_str.upper() == \"REAL\":\n",
    "                return 1\n",
    "            elif assessment_str == \"0\" or assessment_str.upper() == \"FAKE\":\n",
    "                return 0\n",
    "        \n",
    "        # Fallback to simple string check\n",
    "        if \"<overall_assessment>1</overall_assessment>\" in verification_xml:\n",
    "            return 1\n",
    "        elif \"<overall_assessment>0</overall_assessment>\" in verification_xml:\n",
    "            return 0\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting assessment: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_dataset_line(line):\n",
    "    \"\"\"\n",
    "    Parse a line from the Uni-Fakkedit-55k dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract text\n",
    "        text_parts = line.split(\"[TEXT]\")\n",
    "        if len(text_parts) != 2:\n",
    "            return None\n",
    "        after_text = text_parts[1]\n",
    "\n",
    "        # Extract objects\n",
    "        text_objects_parts = after_text.split(\"[OBJECTS]\")\n",
    "        if len(text_objects_parts) != 2:\n",
    "            return None\n",
    "        text_part = text_objects_parts[0].strip()\n",
    "        after_objects = text_objects_parts[1]\n",
    "\n",
    "        # Extract label\n",
    "        objects_label_parts = after_objects.split(\"[LABEL]\")\n",
    "        if len(objects_label_parts) != 2:\n",
    "            return None\n",
    "        objects_part = objects_label_parts[0].strip()\n",
    "        label_part = objects_label_parts[1].strip()\n",
    "\n",
    "        # Combine text and objects - keep it short\n",
    "        combined_input = f\"{text_part}. Objects: {objects_part}\"\n",
    "        label_int = int(label_part)\n",
    "        \n",
    "        return (combined_input, label_int)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing line: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_entry(client, line):\n",
    "    \"\"\"\n",
    "    Process a single dataset entry with the two-agent approach only\n",
    "    \"\"\"\n",
    "    parsed_data = parse_dataset_line(line)\n",
    "    if parsed_data is None:\n",
    "        return None\n",
    "    \n",
    "    combined_input, true_label = parsed_data\n",
    "    \n",
    "    # Test with two-agent approach\n",
    "    two_agent_result = two_agent_detection(client, combined_input)\n",
    "    two_agent_prediction = extract_assessment(two_agent_result[\"verification\"])\n",
    "    two_agent_correct = (two_agent_prediction == true_label) if two_agent_prediction is not None else False\n",
    "    \n",
    "    # Return only the fields we need for the CSV\n",
    "    return {\n",
    "        'true_label': true_label,\n",
    "        'two_agent_prediction': two_agent_prediction,\n",
    "        'two_agent_correct': two_agent_correct,\n",
    "        'two_agent_time': two_agent_result[\"processing_time\"]\n",
    "    }\n",
    "\n",
    "def process_batch(client, batch_lines, lock, writer=None):\n",
    "    \"\"\"\n",
    "    Process a batch of dataset lines\n",
    "    \"\"\"\n",
    "    batch_results = []\n",
    "    \n",
    "    for line in batch_lines:\n",
    "        result = process_entry(client, line)\n",
    "        if result is not None:\n",
    "            batch_results.append(result)\n",
    "            \n",
    "            # Write result to CSV if provided with a writer\n",
    "            if writer:\n",
    "                with lock:\n",
    "                    writer.writerow(result)\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "def run_ablation_study(client, dataset_path, output_path, num_samples=500, batch_size=5, max_workers=2):\n",
    "    \"\"\"\n",
    "    Run a study using the 2-Agent approach only\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Read the dataset lines\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        lines = []\n",
    "        for _ in range(num_samples):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append(line)\n",
    "    \n",
    "    # Create CSV file and writer\n",
    "    fieldnames = [\n",
    "        'true_label', 'two_agent_prediction', 'two_agent_correct', 'two_agent_time'\n",
    "    ]\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    csvfile = open(output_path, 'w', newline='')\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Create a thread lock for CSV writing\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    # Process batches with ThreadPoolExecutor for parallel processing\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit batch processing tasks\n",
    "        futures = []\n",
    "        for i in range(0, len(lines), batch_size):\n",
    "            batch_lines = lines[i:i+batch_size]\n",
    "            futures.append(executor.submit(process_batch, client, batch_lines, lock, writer))\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n",
    "            batch_results = future.result()\n",
    "            results.extend(batch_results)\n",
    "    \n",
    "    # Close CSV file\n",
    "    csvfile.close()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_processed = len(results)\n",
    "    \n",
    "    two_agent_correct = sum(1 for r in results if r['two_agent_correct'])\n",
    "    two_agent_accuracy = two_agent_correct / total_processed if total_processed > 0 else 0\n",
    "    two_agent_avg_time = sum(r['two_agent_time'] for r in results) / total_processed if total_processed > 0 else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n=== STUDY RESULTS ===\")\n",
    "    print(f\"Total samples processed: {total_processed}\")\n",
    "    \n",
    "    print(f\"\\nTWO-AGENT APPROACH\")\n",
    "    print(f\"   Accuracy: {two_agent_accuracy:.4f} ({two_agent_correct}/{total_processed})\")\n",
    "    print(f\"   Avg processing time: {two_agent_avg_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_path}\")\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'two_agent_accuracy': two_agent_accuracy,\n",
    "        'two_agent_avg_time': two_agent_avg_time\n",
    "    }\n",
    "\n",
    "# The client is initialized separately, so we can directly use it\n",
    "dataset_path = r\"C:\\Users\\CoolA\\Code\\dataset_2way_output.txt\"\n",
    "output_path=\"results/3_piece_study_results.csv\"\n",
    "num_samples=500\n",
    "\n",
    "print(f\"Starting study with {num_samples} samples from {dataset_path}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the study\n",
    "metrics = run_ablation_study(client, dataset_path, output_path, num_samples)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nStudy completed in {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting study with 500 samples from C:\\Users\\CoolA\\Code\\dataset_2way_output.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  56%|█████▌    | 56/100 [33:51<33:10, 45.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in Meta-Verifier: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  86%|████████▌ | 86/100 [52:43<11:40, 50.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in Meta-Verifier: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error in Verifier: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error in FactBase: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 100/100 [1:01:40<00:00, 37.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STUDY RESULTS ===\n",
      "Total samples processed: 500\n",
      "\n",
      "THREE-AGENT APPROACH\n",
      "   Accuracy: 0.5740 (287/500)\n",
      "   Avg processing time: 14.78s\n",
      "   Avg confidence: 0.3575\n",
      "\n",
      "Results saved to results/three_agent_study_results.csv\n",
      "\n",
      "Study completed in 3700.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import concurrent.futures\n",
    "import re\n",
    "import threading\n",
    "from functools import lru_cache\n",
    "\n",
    "# Regular expression to extract assessment value\n",
    "ASSESSMENT_PATTERN = re.compile(r'<(overall_assessment|final_assessment)>([01]|REAL|FAKE)</(overall_assessment|final_assessment)>')\n",
    "\n",
    "def three_agent_detection(client, news_text):\n",
    "    \"\"\"\n",
    "    Three-agent framework for fake news detection\n",
    "    \n",
    "    Args:\n",
    "        client: The Gemini API client\n",
    "        news_text: The news text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with factbase output, verification, meta-verification result and processing time\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: FactBase (Evidence Collector) using Gemini 1.5 Pro\n",
    "    factbase_prompt = f\"\"\"\n",
    "    You are FactBase, an expert fact extraction system. Your task is to systematically analyze news content and extract all relevant factual claims, entities, and contextual elements.\n",
    "\n",
    "    Given a news piece, extract:\n",
    "\n",
    "    1. CORE CLAIMS: The central factual assertions (who, what, when, where, how)\n",
    "    2. ENTITIES: People, organizations, locations mentioned\n",
    "    3. SOURCES: Attribution of information (direct quotes, referenced studies, etc.)\n",
    "    4. STATISTICS: Any numerical data or statistics presented\n",
    "    5. TEMPORAL ELEMENTS: Dates, timeframes, sequence of events\n",
    "    6. LOGICAL STRUCTURE: How claims relate to one another\n",
    "\n",
    "    For each element, assign:\n",
    "    - Specificity score (0-1): How precise and verifiable the element is\n",
    "    - Internal consistency score (0-1): How well it aligns with other elements in the text\n",
    "\n",
    "    Format your output as structured JSON with these exact fields:\n",
    "    {{\n",
    "      \"core_claims\": [{{\"claim\": \"text\", \"specificity\": 0.0, \"consistency\": 0.0}}],\n",
    "      \"entities\": [{{\"entity\": \"name\", \"type\": \"person/org/location\", \"context\": \"brief context\"}}],\n",
    "      \"sources\": [{{\"source\": \"name\", \"attribution\": \"how information is attributed\", \"verifiability\": 0.0}}],\n",
    "      \"statistics\": [{{\"claim\": \"statistical claim\", \"specificity\": 0.0}}],\n",
    "      \"temporal_framework\": {{\"timeframe\": \"overall timeframe\", \"consistency\": 0.0}},\n",
    "      \"red_flags\": [\"list any immediate inconsistencies, exaggerations, or vague claims\"]\n",
    "    }}\n",
    "\n",
    "    News to analyze:\n",
    "    {news_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response1 = client.models.generate_content(\n",
    "            model='gemini-1.5-pro',\n",
    "            contents=factbase_prompt\n",
    "        )\n",
    "        factbase_output = response1.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in FactBase: {e}\")\n",
    "        factbase_output = \"\"\"{\"core_claims\": [], \"entities\": [], \"sources\": [], \"statistics\": [], \"temporal_framework\": {\"timeframe\": \"unknown\", \"consistency\": 0.0}, \"red_flags\": [\"Error processing input\"]}\"\"\"\n",
    "    \n",
    "    # Step 2: Verifier (Analysis Engine) using Gemini 2.0 Flash\n",
    "    verify_prompt = f\"\"\"\n",
    "    You are Verifier, an expert news verification system. Your task is to determine whether a news piece is genuine or fabricated, using systematic analysis of evidence and application of media literacy principles.\n",
    "\n",
    "    First, review all information:\n",
    "\n",
    "    1. Original news text:\n",
    "    {news_text}\n",
    "\n",
    "    2. Evidence extracted by FactBase:\n",
    "    {factbase_output}\n",
    "\n",
    "    Now, perform a comprehensive verification using these specific criteria:\n",
    "\n",
    "    1. CLAIM SPECIFICITY: Genuine news typically contains specific, verifiable claims\n",
    "       Score: 0-1 (higher = more specific claims)\n",
    "       \n",
    "    2. SOURCE CREDIBILITY: Genuine news cites specific, checkable sources\n",
    "       Score: 0-1 (higher = more credible sourcing)\n",
    "       \n",
    "    3. INTERNAL CONSISTENCY: Facts should align logically; contradictions suggest fabrication\n",
    "       Score: 0-1 (higher = more internally consistent)\n",
    "       \n",
    "    4. LANGUAGE PATTERNS: Analyze for sensationalism, emotional manipulation, or narrative framing\n",
    "       Score: 0-1 (higher = more neutral language)\n",
    "       \n",
    "    5. CONTEXTUAL COMPLETENESS: Real news provides necessary context; fake news often omits key details\n",
    "       Score: 0-1 (higher = more complete context)\n",
    "\n",
    "    Calculate a weighted verification score using this formula:\n",
    "    Final Score = (Claim_Specificity*0.25 + Source_Credibility*0.25 + Internal_Consistency*0.2 + Language_Patterns*0.15 + Contextual_Completeness*0.15)\n",
    "\n",
    "    Respond using exactly this format:\n",
    "    <verification>\n",
    "      <overall_assessment>{{0 for FAKE, 1 for REAL - use 0.5 threshold on final score}}</overall_assessment>\n",
    "      <confidence_score>{{final_score}}</confidence_score>\n",
    "      <key_issues>{{3 most significant red flags or strongest verification points}}</key_issues>\n",
    "      <reasoning>{{concise explanation focusing on the strongest evidence for your conclusion}}</reasoning>\n",
    "    </verification>\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response2 = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=verify_prompt\n",
    "        )\n",
    "        verification = response2.text\n",
    "        \n",
    "        # Ensure it has the required XML format\n",
    "        if not \"<verification>\" in verification:\n",
    "            verification = f\"\"\"<verification>\n",
    "  <overall_assessment>0</overall_assessment>\n",
    "  <confidence_score>0.5</confidence_score>\n",
    "  <key_issues>Malformed response</key_issues>\n",
    "  <reasoning>Could not properly analyze the content</reasoning>\n",
    "</verification>\"\"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Verifier: {e}\")\n",
    "        verification = \"\"\"<verification>\n",
    "  <overall_assessment>0</overall_assessment>\n",
    "  <confidence_score>0.0</confidence_score>\n",
    "  <key_issues>Error processing input</key_issues>\n",
    "  <reasoning>Verifier encountered an error</reasoning>\n",
    "</verification>\"\"\"\n",
    "    \n",
    "    # Step 3: Meta-Verifier (Ensemble Approach) using Gemini 1.5 Pro for deeper analysis\n",
    "    meta_prompt = f\"\"\"\n",
    "    You are Meta-Verifier, a specialized system designed to make final determinations about news authenticity. You've been given multiple assessments and must synthesize them into a final verdict.\n",
    "\n",
    "    Review all information:\n",
    "\n",
    "    1. Original news text:\n",
    "    {news_text}\n",
    "\n",
    "    2. FactBase evidence:\n",
    "    {factbase_output}\n",
    "\n",
    "    3. Primary verification:\n",
    "    {verification}\n",
    "\n",
    "    Now perform a higher-order analysis:\n",
    "\n",
    "    1. LINGUISTIC RED FLAGS (detect for manipulative language, unusual patterns, or propaganda techniques)\n",
    "       - Emotionally charged words\n",
    "       - Black/white thinking\n",
    "       - Unusual formatting or structure\n",
    "       \n",
    "    2. FACTUAL DENSITY ANALYSIS (evaluate the ratio of verifiable facts to opinion/commentary)\n",
    "\n",
    "    3. SOURCE ANALYSIS (evaluate how information is attributed and sourced)\n",
    "\n",
    "    4. COHERENCE ASSESSMENT (examine logical flow and contextual alignment)\n",
    "\n",
    "    5. PLAUSIBILITY CHECK (evaluate if claims align with fundamental knowledge)\n",
    "\n",
    "    Assign a final verification confidence, potentially overriding previous assessments if strong evidence exists.\n",
    "\n",
    "    Respond using exactly this format with no additional text:\n",
    "    <final_verification>\n",
    "      <overall_assessment>{{0 for FAKE, 1 for REAL}}</overall_assessment>\n",
    "      <confidence_score>{{0-1}}</confidence_score>\n",
    "      <key_issues>{{1-3 most critical indicators}}</key_issues>\n",
    "      <reasoning>{{concise explanation of final determination}}</reasoning>\n",
    "    </final_verification>\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response3 = client.models.generate_content(\n",
    "            model='gemini-1.5-pro',  # Using 1.5 Pro for meta-verification for deeper analysis\n",
    "            contents=meta_prompt\n",
    "        )\n",
    "        meta_verification = response3.text\n",
    "        \n",
    "        # Ensure it has the required XML format\n",
    "        if not \"<final_verification>\" in meta_verification:\n",
    "            meta_verification = f\"\"\"<final_verification>\n",
    "  <overall_assessment>0</overall_assessment>\n",
    "  <confidence_score>0.5</confidence_score>\n",
    "  <key_issues>Malformed response</key_issues>\n",
    "  <reasoning>Could not properly perform meta-analysis</reasoning>\n",
    "</final_verification>\"\"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Meta-Verifier: {e}\")\n",
    "        meta_verification = \"\"\"<final_verification>\n",
    "  <overall_assessment>0</overall_assessment>\n",
    "  <confidence_score>0.0</confidence_score>\n",
    "  <key_issues>Error processing input</key_issues>\n",
    "  <reasoning>Meta-Verifier encountered an error</reasoning>\n",
    "</final_verification>\"\"\"\n",
    "        \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "        \n",
    "    return {\n",
    "        \"factbase_output\": factbase_output,\n",
    "        \"verification\": verification,\n",
    "        \"meta_verification\": meta_verification,\n",
    "        \"processing_time\": processing_time\n",
    "    }\n",
    "\n",
    "def extract_assessment(verification_xml):\n",
    "    \"\"\"\n",
    "    Extract the overall assessment (1 or 0) from the verification XML\n",
    "    Works with both normal verification and meta-verification formats\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regex pattern for faster extraction\n",
    "        match = ASSESSMENT_PATTERN.search(verification_xml)\n",
    "        if match:\n",
    "            assessment_str = match.group(2)\n",
    "            if assessment_str == \"1\" or assessment_str.upper() == \"REAL\":\n",
    "                return 1\n",
    "            elif assessment_str == \"0\" or assessment_str.upper() == \"FAKE\":\n",
    "                return 0\n",
    "        \n",
    "        # Fallback to simple string check\n",
    "        if \"<overall_assessment>1</overall_assessment>\" in verification_xml or \"<final_assessment>1</final_assessment>\" in verification_xml:\n",
    "            return 1\n",
    "        elif \"<overall_assessment>0</overall_assessment>\" in verification_xml or \"<final_assessment>0</final_assessment>\" in verification_xml:\n",
    "            return 0\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting assessment: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_dataset_line(line):\n",
    "    \"\"\"\n",
    "    Parse a line from the Uni-Fakkedit-55k dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract text\n",
    "        text_parts = line.split(\"[TEXT]\")\n",
    "        if len(text_parts) != 2:\n",
    "            return None\n",
    "        after_text = text_parts[1]\n",
    "\n",
    "        # Extract objects\n",
    "        text_objects_parts = after_text.split(\"[OBJECTS]\")\n",
    "        if len(text_objects_parts) != 2:\n",
    "            return None\n",
    "        text_part = text_objects_parts[0].strip()\n",
    "        after_objects = text_objects_parts[1]\n",
    "\n",
    "        # Extract label\n",
    "        objects_label_parts = after_objects.split(\"[LABEL]\")\n",
    "        if len(objects_label_parts) != 2:\n",
    "            return None\n",
    "        objects_part = objects_label_parts[0].strip()\n",
    "        label_part = objects_label_parts[1].strip()\n",
    "\n",
    "        # Combine text and objects - keep it short\n",
    "        combined_input = f\"{text_part}. Objects: {objects_part}\"\n",
    "        label_int = int(label_part)\n",
    "        \n",
    "        return (combined_input, label_int)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing line: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_entry(client, line):\n",
    "    \"\"\"\n",
    "    Process a single dataset entry with the three-agent approach\n",
    "    \"\"\"\n",
    "    parsed_data = parse_dataset_line(line)\n",
    "    if parsed_data is None:\n",
    "        return None\n",
    "    \n",
    "    combined_input, true_label = parsed_data\n",
    "    \n",
    "    # Test with three-agent approach\n",
    "    three_agent_result = three_agent_detection(client, combined_input)\n",
    "    three_agent_prediction = extract_assessment(three_agent_result[\"meta_verification\"])\n",
    "    three_agent_correct = (three_agent_prediction == true_label) if three_agent_prediction is not None else False\n",
    "    \n",
    "    # Extract confidence score\n",
    "    confidence_pattern = re.compile(r'<confidence_score>([\\d\\.]+)</confidence_score>')\n",
    "    match = confidence_pattern.search(three_agent_result[\"meta_verification\"])\n",
    "    confidence = float(match.group(1)) if match else 0.0\n",
    "    \n",
    "    # Return only the fields we need for the CSV\n",
    "    return {\n",
    "        'true_label': true_label,\n",
    "        'three_agent_prediction': three_agent_prediction,\n",
    "        'three_agent_correct': three_agent_correct,\n",
    "        'three_agent_time': three_agent_result[\"processing_time\"],\n",
    "        'three_agent_confidence': confidence\n",
    "    }\n",
    "\n",
    "def process_batch(client, batch_lines, lock, writer=None):\n",
    "    \"\"\"\n",
    "    Process a batch of dataset lines\n",
    "    \"\"\"\n",
    "    batch_results = []\n",
    "    \n",
    "    for line in batch_lines:\n",
    "        result = process_entry(client, line)\n",
    "        if result is not None:\n",
    "            batch_results.append(result)\n",
    "            \n",
    "            # Write result to CSV if provided with a writer\n",
    "            if writer:\n",
    "                with lock:\n",
    "                    writer.writerow(result)\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "def run_study(client, dataset_path, output_path, num_samples=500, batch_size=5, max_workers=2):\n",
    "    \"\"\"\n",
    "    Run a study using the 3-Agent approach\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Read the dataset lines\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        lines = []\n",
    "        for _ in range(num_samples):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append(line)\n",
    "    \n",
    "    # Create CSV file and writer\n",
    "    fieldnames = [\n",
    "        'true_label', 'three_agent_prediction', 'three_agent_correct', \n",
    "        'three_agent_time', 'three_agent_confidence'\n",
    "    ]\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    csvfile = open(output_path, 'w', newline='')\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Create a thread lock for CSV writing\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    # Process batches with ThreadPoolExecutor for parallel processing\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit batch processing tasks\n",
    "        futures = []\n",
    "        for i in range(0, len(lines), batch_size):\n",
    "            batch_lines = lines[i:i+batch_size]\n",
    "            futures.append(executor.submit(process_batch, client, batch_lines, lock, writer))\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n",
    "            batch_results = future.result()\n",
    "            results.extend(batch_results)\n",
    "    \n",
    "    # Close CSV file\n",
    "    csvfile.close()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_processed = len(results)\n",
    "    \n",
    "    three_agent_correct = sum(1 for r in results if r['three_agent_correct'])\n",
    "    three_agent_accuracy = three_agent_correct / total_processed if total_processed > 0 else 0\n",
    "    three_agent_avg_time = sum(r['three_agent_time'] for r in results) / total_processed if total_processed > 0 else 0\n",
    "    three_agent_avg_confidence = sum(r['three_agent_confidence'] for r in results) / total_processed if total_processed > 0 else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n=== STUDY RESULTS ===\")\n",
    "    print(f\"Total samples processed: {total_processed}\")\n",
    "    \n",
    "    print(f\"\\nTHREE-AGENT APPROACH\")\n",
    "    print(f\"   Accuracy: {three_agent_accuracy:.4f} ({three_agent_correct}/{total_processed})\")\n",
    "    print(f\"   Avg processing time: {three_agent_avg_time:.2f}s\")\n",
    "    print(f\"   Avg confidence: {three_agent_avg_confidence:.4f}\")\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_path}\")\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'three_agent_accuracy': three_agent_accuracy,\n",
    "        'three_agent_avg_time': three_agent_avg_time,\n",
    "        'three_agent_avg_confidence': three_agent_avg_confidence\n",
    "    }\n",
    "\n",
    "# The main execution code\n",
    "if __name__ == \"__main__\":\n",
    "    # The client is initialized separately, so we can directly use it\n",
    "    dataset_path = r\"C:\\Users\\CoolA\\Code\\dataset_2way_output.txt\"\n",
    "    output_path = \"results/three_agent_study_results.csv\"\n",
    "    num_samples = 500\n",
    "\n",
    "    print(f\"Starting study with {num_samples} samples from {dataset_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run the study\n",
    "    metrics = run_study(client, dataset_path, output_path, num_samples)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\nStudy completed in {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ABLATION STUDY RESULTS ===\n",
      "Total samples processed: 487\n",
      "\n",
      "1. GEMINI 1.5 PRO\n",
      "   Accuracy: 0.6879 (335/487)\n",
      "   Avg processing time: 4.78s\n",
      "\n",
      "2. GEMINI 2.0 FLASH\n",
      "   Accuracy: 0.7269 (354/487)\n",
      "   Avg processing time: 2.96s\n",
      "\n",
      "3. TWO-AGENT APPROACH\n",
      "   Accuracy: 0.6694 (326/487)\n",
      "   Avg processing time: 6.74s\n",
      "\n",
      "=== PERFORMANCE COMPARISON ===\n",
      "Best single model: Gemini 2.0 Flash (0.7269)\n",
      "Two-agent approach: 0.6694\n",
      "Absolute improvement: -0.0575\n",
      "Relative improvement: -7.91%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r\"C:\\Users\\CoolA\\Code\\results\\final_results.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate total processed samples\n",
    "total_processed = len(df)\n",
    "\n",
    "# Compute accuracy and average processing time\n",
    "pro_correct = df[\"pro_correct\"].sum()\n",
    "pro_accuracy = pro_correct / total_processed\n",
    "pro_avg_time = df[\"pro_time\"].mean()\n",
    "\n",
    "flash_correct = df[\"flash_correct\"].sum()\n",
    "flash_accuracy = flash_correct / total_processed\n",
    "flash_avg_time = df[\"flash_time\"].mean()\n",
    "\n",
    "two_agent_correct = df[\"two_agent_correct\"].sum()\n",
    "two_agent_accuracy = two_agent_correct / total_processed\n",
    "two_agent_avg_time = df[\"two_agent_time\"].mean()\n",
    "\n",
    "# Calculate performance improvement\n",
    "best_single_accuracy = max(pro_accuracy, flash_accuracy)\n",
    "best_single_model = \"Gemini 1.5 Pro\" if pro_accuracy > flash_accuracy else \"Gemini 2.0 Flash\"\n",
    "\n",
    "accuracy_improvement = two_agent_accuracy - best_single_accuracy\n",
    "relative_improvement = (accuracy_improvement / best_single_accuracy) * 100 if best_single_accuracy > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n=== ABLATION STUDY RESULTS ===\")\n",
    "print(f\"Total samples processed: {total_processed}\")\n",
    "\n",
    "print(f\"\\n1. GEMINI 1.5 PRO\")\n",
    "print(f\"   Accuracy: {pro_accuracy:.4f} ({pro_correct}/{total_processed})\")\n",
    "print(f\"   Avg processing time: {pro_avg_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n2. GEMINI 2.0 FLASH\")\n",
    "print(f\"   Accuracy: {flash_accuracy:.4f} ({flash_correct}/{total_processed})\")\n",
    "print(f\"   Avg processing time: {flash_avg_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n3. TWO-AGENT APPROACH\")\n",
    "print(f\"   Accuracy: {two_agent_accuracy:.4f} ({two_agent_correct}/{total_processed})\")\n",
    "print(f\"   Avg processing time: {two_agent_avg_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "print(f\"Best single model: {best_single_model} ({best_single_accuracy:.4f})\")\n",
    "print(f\"Two-agent approach: {two_agent_accuracy:.4f}\")\n",
    "print(f\"Absolute improvement: {accuracy_improvement:.4f}\")\n",
    "print(f\"Relative improvement: {relative_improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing class distribution...\n",
      "\n",
      "Class Distribution:\n",
      "Class 0 (True): 219 samples\n",
      "Class 1 (False Content): 281 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 44/44 [00:02<00:00, 18.51it/s, loss=0.8050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Training Loss: 0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Eval]: 100%|██████████| 19/19 [00:00<00:00, 66.84it/s, loss=0.7881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation Results:\n",
      "  Loss: 0.7569\n",
      "  Accuracy: 0.5600\n",
      "  Macro F1: 0.3590\n",
      "  Weighted F1: 0.4021\n",
      "  New best model saved with F1 Macro: 0.3590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 44/44 [00:02<00:00, 20.10it/s, loss=0.6426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Training Loss: 0.7290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Eval]: 100%|██████████| 19/19 [00:00<00:00, 64.99it/s, loss=0.7687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation Results:\n",
      "  Loss: 0.6783\n",
      "  Accuracy: 0.5867\n",
      "  Macro F1: 0.4224\n",
      "  Weighted F1: 0.4593\n",
      "  New best model saved with F1 Macro: 0.4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 44/44 [00:02<00:00, 20.20it/s, loss=0.5902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Training Loss: 0.6067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Eval]: 100%|██████████| 19/19 [00:00<00:00, 65.27it/s, loss=0.5183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation Results:\n",
      "  Loss: 0.5760\n",
      "  Accuracy: 0.7600\n",
      "  Macro F1: 0.7600\n",
      "  Weighted F1: 0.7600\n",
      "  New best model saved with F1 Macro: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 44/44 [00:02<00:00, 20.09it/s, loss=0.5220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average Training Loss: 0.4538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Eval]: 100%|██████████| 19/19 [00:00<00:00, 63.51it/s, loss=0.6905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Evaluation Results:\n",
      "  Loss: 0.5393\n",
      "  Accuracy: 0.7400\n",
      "  Macro F1: 0.7232\n",
      "  Weighted F1: 0.7314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 44/44 [00:02<00:00, 20.03it/s, loss=0.2661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Average Training Loss: 0.3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Eval]: 100%|██████████| 19/19 [00:00<00:00, 64.66it/s, loss=0.7823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Evaluation Results:\n",
      "  Loss: 0.5577\n",
      "  Accuracy: 0.7533\n",
      "  Macro F1: 0.7312\n",
      "  Weighted F1: 0.7405\n",
      "Loaded best model from ./best_model_custom.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 19/19 [00:00<00:00, 69.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Metrics:\n",
      "Accuracy: 0.7600\n",
      "Macro-averaged Precision: 0.7711\n",
      "Weighted-averaged Precision: 0.7822\n",
      "Macro-averaged F1: 0.7600\n",
      "Weighted-averaged F1: 0.7600\n",
      "\n",
      "Per-class Precision:\n",
      "Class 0 (True): 0.6786\n",
      "Class 1 (False Content): 0.8636\n",
      "\n",
      "Per-class F1:\n",
      "Class 0 (True): 0.7600\n",
      "Class 1 (False Content): 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Class name mapping\n",
    "CLASS_NAMES = {\n",
    "    0: \"True\",\n",
    "    1: \"False Content\",\n",
    "    #2: \"Misleading Content\",\n",
    "    #3: \"Manipulated Content\",\n",
    "    #4: \"False Content\",\n",
    "    #5: \"Imposter Content\"\n",
    "}\n",
    "\n",
    "# Function to analyze class distribution\n",
    "def analyze_class_distribution(labels):\n",
    "    # Count instances of each class\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pie([class_counts.get(i, 0) for i in range(6)], \n",
    "            labels=[f\"{CLASS_NAMES.get(i, f'Class {i}')}\\n({class_counts.get(i, 0)} samples)\" for i in range(6)],\n",
    "            autopct='%1.1f%%')\n",
    "    plt.title('Distribution of Classes in Dataset')\n",
    "    plt.axis('equal')\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    for i in range(2):\n",
    "        print(f\"Class {i} ({CLASS_NAMES.get(i, f'Class {i}')}): {class_counts.get(i, 0)} samples\")\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(predictions, labels):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate macro-averaged metrics (treats all classes equally) \n",
    "    precision_macro = precision_score(labels, predictions, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    # Calculate weighted-averaged metrics (accounts for class imbalance)\n",
    "    precision_weighted = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    precision_per_class = precision_score(labels, predictions, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(labels, predictions, average=None)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"precision_weighted\": precision_weighted,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_per_class\": precision_per_class.tolist(),\n",
    "        \"f1_per_class\": f1_per_class.tolist()\n",
    "    }\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Path to the dataset\n",
    "data_path = r\"C:\\Users\\CoolA\\Code\\extracted_500.txt\"\n",
    "\n",
    "# Load the dataset\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Parse the line\n",
    "        parts = line.split(\"[TEXT]\")\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "        after_text = parts[1]\n",
    "        \n",
    "        text_objects_parts = after_text.split(\"[OBJECTS]\")\n",
    "        if len(text_objects_parts) != 2:\n",
    "            continue\n",
    "        text_part = text_objects_parts[0].strip()\n",
    "        after_objects = text_objects_parts[1]\n",
    "        \n",
    "        objects_label_parts = after_objects.split(\"[LABEL]\")\n",
    "        if len(objects_label_parts) != 2:\n",
    "            continue\n",
    "        objects_part = objects_label_parts[0].strip()\n",
    "        label_part = objects_label_parts[1].strip()\n",
    "        \n",
    "        label_int = int(label_part)\n",
    "        \n",
    "        # Convert the objects_part (comma-separated) into a Python list\n",
    "        objects_list = [obj.strip() for obj in objects_part.split(\",\") if obj.strip()]\n",
    "        \n",
    "        # Build a short descriptive sentence for the objects\n",
    "        if len(objects_list) > 0:\n",
    "            object_sentence = f\"The image contains: {', '.join(objects_list)}.\"\n",
    "        else:\n",
    "            object_sentence = \"No objects detected.\"\n",
    "        \n",
    "        # Combine the main text with the object sentence\n",
    "        combined_input = f\"{text_part}. {object_sentence}\"\n",
    "        \n",
    "        texts.append(combined_input)\n",
    "        labels.append(label_int)\n",
    "\n",
    "# Split into train and test (70/30)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Analyze class distribution\n",
    "print(\"Analyzing class distribution...\")\n",
    "analyze_class_distribution(labels)\n",
    "\n",
    "# Load a BERT tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the datasets\n",
    "max_length = 128\n",
    "\n",
    "# Tokenize training data\n",
    "train_encodings = tokenizer(\n",
    "    train_texts, \n",
    "    truncation=True, \n",
    "    padding=\"max_length\", \n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Tokenize test data\n",
    "test_encodings = tokenizer(\n",
    "    test_texts, \n",
    "    truncation=True, \n",
    "    padding=\"max_length\", \n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(\n",
    "    train_encodings['input_ids'],\n",
    "    train_encodings['attention_mask'],\n",
    "    train_labels\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    test_encodings['input_ids'],\n",
    "    test_encodings['attention_mask'],\n",
    "    test_labels\n",
    ")\n",
    "\n",
    "# Create DataLoaders \n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "# Setup GPU/CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# If using multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "\n",
    "# Training loop\n",
    "best_f1 = 0.0\n",
    "best_model_path = \"./best_model_custom.pt\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for batch in progress_bar:\n",
    "        # Get batch data\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    eval_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Eval]\")\n",
    "        for batch in progress_bar:\n",
    "            # Get batch data\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Accumulate loss\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Calculate average evaluation loss\n",
    "    avg_eval_loss = eval_loss / len(test_loader)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(f\"Epoch {epoch+1} - Evaluation Results:\")\n",
    "    print(f\"  Loss: {avg_eval_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Macro F1: {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  Weighted F1: {metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    if metrics['f1_macro'] > best_f1:\n",
    "        best_f1 = metrics['f1_macro']\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            torch.save(model.module.state_dict(), best_model_path)\n",
    "        else:\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"  New best model saved with F1 Macro: {best_f1:.4f}\")\n",
    "\n",
    "# Load the best model for final evaluation\n",
    "if os.path.exists(best_model_path):\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(torch.load(best_model_path))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "    print(f\"Loaded best model from {best_model_path}\")\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute and print final metrics\n",
    "final_metrics = compute_metrics(all_preds, all_labels)\n",
    "\n",
    "print(\"\\nFinal Test Set Metrics:\")\n",
    "print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"Macro-averaged Precision: {final_metrics['precision_macro']:.4f}\")\n",
    "print(f\"Weighted-averaged Precision: {final_metrics['precision_weighted']:.4f}\")\n",
    "print(f\"Macro-averaged F1: {final_metrics['f1_macro']:.4f}\")\n",
    "print(f\"Weighted-averaged F1: {final_metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class Precision:\")\n",
    "for i, p in enumerate(final_metrics['precision_per_class']):\n",
    "    if i in CLASS_NAMES:\n",
    "        print(f\"Class {i} ({CLASS_NAMES[i]}): {p:.4f}\")\n",
    "    else:\n",
    "        print(f\"Class {i}: {p:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class F1:\")\n",
    "for i, f in enumerate(final_metrics['f1_per_class']):\n",
    "    if i in CLASS_NAMES:\n",
    "        print(f\"Class {i} ({CLASS_NAMES[i]}): {f:.4f}\")\n",
    "    else:\n",
    "        print(f\"Class {i}: {f:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Class name mapping - updated for binary classification\n",
    "CLASS_NAMES = {\n",
    "    0: \"True\",\n",
    "    1: \"False Content\"\n",
    "}\n",
    "\n",
    "# Function to analyze class distribution\n",
    "def analyze_class_distribution(labels):\n",
    "    # Count instances of each class\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pie([class_counts.get(i, 0) for i in range(2)], \n",
    "            labels=[f\"{CLASS_NAMES.get(i, f'Class {i}')}\\n({class_counts.get(i, 0)} samples)\" for i in range(2)],\n",
    "            autopct='%1.1f%%')\n",
    "    plt.title('Distribution of Classes in Dataset')\n",
    "    plt.axis('equal')\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    for i in range(2):\n",
    "        print(f\"Class {i} ({CLASS_NAMES.get(i, f'Class {i}')}): {class_counts.get(i, 0)} samples\")\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(predictions, labels):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate macro-averaged metrics (treats all classes equally) \n",
    "    precision_macro = precision_score(labels, predictions, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    # Calculate weighted-averaged metrics (accounts for class imbalance)\n",
    "    precision_weighted = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    precision_per_class = precision_score(labels, predictions, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(labels, predictions, average=None)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"precision_weighted\": precision_weighted,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_per_class\": precision_per_class.tolist(),\n",
    "        \"f1_per_class\": f1_per_class.tolist()\n",
    "    }\n",
    "\n",
    "# Main function with model type parameter\n",
    "def small_model_trainer(model_type=\"tinybert\"):\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Path to the dataset\n",
    "    data_path = r\"C:\\Users\\CoolA\\Code\\extracted_500.txt\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Parse the line\n",
    "            parts = line.split(\"[TEXT]\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            after_text = parts[1]\n",
    "            \n",
    "            text_objects_parts = after_text.split(\"[OBJECTS]\")\n",
    "            if len(text_objects_parts) != 2:\n",
    "                continue\n",
    "            text_part = text_objects_parts[0].strip()\n",
    "            after_objects = text_objects_parts[1]\n",
    "            \n",
    "            objects_label_parts = after_objects.split(\"[LABEL]\")\n",
    "            if len(objects_label_parts) != 2:\n",
    "                continue\n",
    "            objects_part = objects_label_parts[0].strip()\n",
    "            label_part = objects_label_parts[1].strip()\n",
    "            \n",
    "            label_int = int(label_part)\n",
    "            \n",
    "            # Convert multi-class to binary (0 = True, 1-5 = False)\n",
    "            binary_label = 0 if label_int == 0 else 1\n",
    "            \n",
    "            # Convert the objects_part (comma-separated) into a Python list\n",
    "            objects_list = [obj.strip() for obj in objects_part.split(\",\") if obj.strip()]\n",
    "            \n",
    "            # Build a short descriptive sentence for the objects\n",
    "            if len(objects_list) > 0:\n",
    "                object_sentence = f\"The image contains: {', '.join(objects_list)}.\"\n",
    "            else:\n",
    "                object_sentence = \"No objects detected.\"\n",
    "            \n",
    "            # Combine the main text with the object sentence\n",
    "            combined_input = f\"{text_part}. {object_sentence}\"\n",
    "            \n",
    "            texts.append(combined_input)\n",
    "            labels.append(binary_label)\n",
    "\n",
    "    # Split into train and test (70/30)\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Analyze class distribution\n",
    "    print(\"Analyzing class distribution...\")\n",
    "    analyze_class_distribution(labels)\n",
    "\n",
    "    # Load model and tokenizer based on model_type\n",
    "    print(f\"\\nInitializing tokenizer and model for {model_type}...\")\n",
    "    \n",
    "    if model_type.lower() == \"tinybert\":\n",
    "        model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,  # 2-way classification\n",
    "            problem_type=\"single_label_classification\"\n",
    "        )\n",
    "    elif model_type.lower() == \"distilbert\":\n",
    "        model_name = \"distilbert-base-uncased\"\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=2  # 2-way classification\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}. Choose 'tinybert' or 'distilbert'.\")\n",
    "        \n",
    "    print(f\"Using {model_name}\")\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    max_length = 128\n",
    "    \n",
    "    # Tokenize training data\n",
    "    print(\"Tokenizing training data...\")\n",
    "    train_encodings = tokenizer(\n",
    "        train_texts, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize test data\n",
    "    print(\"Tokenizing test data...\")\n",
    "    test_encodings = tokenizer(\n",
    "        test_texts, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Convert labels to tensors\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    \n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = TensorDataset(\n",
    "        train_encodings['input_ids'],\n",
    "        train_encodings['attention_mask'],\n",
    "        train_labels\n",
    "    )\n",
    "    \n",
    "    test_dataset = TensorDataset(\n",
    "        test_encodings['input_ids'],\n",
    "        test_encodings['attention_mask'],\n",
    "        test_labels\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders - optimize batch size for smaller models\n",
    "    batch_size = 16  # Increased batch size for faster training with smaller models\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Setup GPU/CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # If using multiple GPUs\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Training parameters - optimize for smaller models\n",
    "    num_epochs = 5\n",
    "    learning_rate = 5e-5  # Slightly higher learning rate for smaller models\n",
    "    weight_decay = 0.01\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler \n",
    "    # Using OneCycleLR for faster convergence with smaller models\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=learning_rate,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0.0\n",
    "    best_model_path = f\"./best_{model_type}_model.pt\"\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for batch in progress_bar:\n",
    "            # Get batch data\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        eval_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Eval]\")\n",
    "            for batch in progress_bar:\n",
    "                # Get batch data\n",
    "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                \n",
    "                # Store predictions and labels\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Accumulate loss\n",
    "                eval_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Calculate average evaluation loss\n",
    "        avg_eval_loss = eval_loss / len(test_loader)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_metrics(all_preds, all_labels)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(f\"Epoch {epoch+1} - Evaluation Results:\")\n",
    "        print(f\"  Loss: {avg_eval_loss:.4f}\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Macro F1: {metrics['f1_macro']:.4f}\")\n",
    "        print(f\"  Weighted F1: {metrics['f1_weighted']:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if metrics['f1_macro'] > best_f1:\n",
    "            best_f1 = metrics['f1_macro']\n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                torch.save(model.module.state_dict(), best_model_path)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"  New best model saved with F1 Macro: {best_f1:.4f}\")\n",
    "    \n",
    "    # Load the best model for final evaluation\n",
    "    if os.path.exists(best_model_path):\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.load_state_dict(torch.load(best_model_path))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"Loaded best model from {best_model_path}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute and print final metrics\n",
    "    final_metrics = compute_metrics(all_preds, all_labels)\n",
    "    \n",
    "    print(\"\\nFinal Test Set Metrics:\")\n",
    "    print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Macro-averaged Precision: {final_metrics['precision_macro']:.4f}\")\n",
    "    print(f\"Weighted-averaged Precision: {final_metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"Macro-averaged F1: {final_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Weighted-averaged F1: {final_metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class Precision:\")\n",
    "    for i, p in enumerate(final_metrics['precision_per_class']):\n",
    "        if i in CLASS_NAMES:\n",
    "            print(f\"Class {i} ({CLASS_NAMES[i]}): {p:.4f}\")\n",
    "        else:\n",
    "            print(f\"Class {i}: {p:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class F1:\")\n",
    "    for i, f in enumerate(final_metrics['f1_per_class']):\n",
    "        if i in CLASS_NAMES:\n",
    "            print(f\"Class {i} ({CLASS_NAMES[i]}): {f:.4f}\")\n",
    "        else:\n",
    "            print(f\"Class {i}: {f:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Running TinyBERT training:\n",
      "Analyzing class distribution...\n",
      "\n",
      "Class Distribution:\n",
      "Class 0 (True): 219 samples\n",
      "Class 1 (False Content): 281 samples\n",
      "\n",
      "Initializing tokenizer and model for tinybert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using huawei-noah/TinyBERT_General_4L_312D\n",
      "Tokenizing training data...\n",
      "Tokenizing test data...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 22/22 [00:00<00:00, 77.36it/s, loss=0.7448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Training Loss: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 150.80it/s, loss=0.6969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation Results:\n",
      "  Loss: 0.6873\n",
      "  Accuracy: 0.5600\n",
      "  Macro F1: 0.3590\n",
      "  Weighted F1: 0.4021\n",
      "  New best model saved with F1 Macro: 0.3590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 22/22 [00:00<00:00, 73.82it/s, loss=0.7032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Training Loss: 0.6890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 207.57it/s, loss=0.6874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation Results:\n",
      "  Loss: 0.6883\n",
      "  Accuracy: 0.5600\n",
      "  Macro F1: 0.3590\n",
      "  Weighted F1: 0.4021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 22/22 [00:00<00:00, 77.50it/s, loss=0.6546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Training Loss: 0.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 195.49it/s, loss=0.6580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation Results:\n",
      "  Loss: 0.6721\n",
      "  Accuracy: 0.6200\n",
      "  Macro F1: 0.6171\n",
      "  Weighted F1: 0.6211\n",
      "  New best model saved with F1 Macro: 0.6171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 22/22 [00:00<00:00, 78.78it/s, loss=0.6204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average Training Loss: 0.6269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 170.26it/s, loss=0.6248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Evaluation Results:\n",
      "  Loss: 0.6494\n",
      "  Accuracy: 0.6533\n",
      "  Macro F1: 0.6523\n",
      "  Weighted F1: 0.6501\n",
      "  New best model saved with F1 Macro: 0.6523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 22/22 [00:00<00:00, 79.59it/s, loss=0.6868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Average Training Loss: 0.6037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 203.72it/s, loss=0.6115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Evaluation Results:\n",
      "  Loss: 0.6421\n",
      "  Accuracy: 0.6333\n",
      "  Macro F1: 0.6296\n",
      "  Weighted F1: 0.6341\n",
      "Loaded best model from ./best_tinybert_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 10/10 [00:00<00:00, 171.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Metrics:\n",
      "Accuracy: 0.6533\n",
      "Macro-averaged Precision: 0.6760\n",
      "Weighted-averaged Precision: 0.6880\n",
      "Macro-averaged F1: 0.6523\n",
      "Weighted-averaged F1: 0.6501\n",
      "\n",
      "Per-class Precision:\n",
      "Class 0 (True): 0.5761\n",
      "Class 1 (False Content): 0.7759\n",
      "\n",
      "Per-class F1:\n",
      "Class 0 (True): 0.6709\n",
      "Class 1 (False Content): 0.6338\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# To run both models sequentially:\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Running TinyBERT training:\")\n",
    "small_model_trainer(\"tinybert\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Running DistilBERT training:\n",
      "Analyzing class distribution...\n",
      "\n",
      "Class Distribution:\n",
      "Class 0 (True): 219 samples\n",
      "Class 1 (False Content): 281 samples\n",
      "\n",
      "Initializing tokenizer and model for distilbert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using distilbert-base-uncased\n",
      "Tokenizing training data...\n",
      "Tokenizing test data...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 22/22 [00:00<00:00, 22.72it/s, loss=0.6783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Training Loss: 0.6888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 70.08it/s, loss=0.6970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation Results:\n",
      "  Loss: 0.6764\n",
      "  Accuracy: 0.5733\n",
      "  Macro F1: 0.4037\n",
      "  Weighted F1: 0.4419\n",
      "  New best model saved with F1 Macro: 0.4037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 22/22 [00:00<00:00, 24.04it/s, loss=0.7400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Training Loss: 0.6410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 76.54it/s, loss=0.7613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation Results:\n",
      "  Loss: 0.6762\n",
      "  Accuracy: 0.6133\n",
      "  Macro F1: 0.4890\n",
      "  Weighted F1: 0.5192\n",
      "  New best model saved with F1 Macro: 0.4890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 22/22 [00:00<00:00, 24.67it/s, loss=0.4641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Training Loss: 0.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 77.49it/s, loss=0.5062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation Results:\n",
      "  Loss: 0.5878\n",
      "  Accuracy: 0.7200\n",
      "  Macro F1: 0.6962\n",
      "  Weighted F1: 0.7064\n",
      "  New best model saved with F1 Macro: 0.6962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 22/22 [00:00<00:00, 24.03it/s, loss=0.0782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average Training Loss: 0.1689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 76.96it/s, loss=0.4713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Evaluation Results:\n",
      "  Loss: 0.7272\n",
      "  Accuracy: 0.6867\n",
      "  Macro F1: 0.6614\n",
      "  Weighted F1: 0.6725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 22/22 [00:00<00:00, 24.66it/s, loss=0.0639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Average Training Loss: 0.0957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 75.49it/s, loss=0.1399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Evaluation Results:\n",
      "  Loss: 0.6906\n",
      "  Accuracy: 0.7200\n",
      "  Macro F1: 0.7182\n",
      "  Weighted F1: 0.7209\n",
      "  New best model saved with F1 Macro: 0.7182\n",
      "Loaded best model from ./best_distilbert_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 10/10 [00:00<00:00, 82.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Metrics:\n",
      "Accuracy: 0.7200\n",
      "Macro-averaged Precision: 0.7179\n",
      "Weighted-averaged Precision: 0.7241\n",
      "Macro-averaged F1: 0.7182\n",
      "Weighted-averaged F1: 0.7209\n",
      "\n",
      "Per-class Precision:\n",
      "Class 0 (True): 0.6667\n",
      "Class 1 (False Content): 0.7692\n",
      "\n",
      "Per-class F1:\n",
      "Class 0 (True): 0.6957\n",
      "Class 1 (False Content): 0.7407\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Running DistilBERT training:\")\n",
    "small_model_trainer(\"distilbert\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Class name mapping - updated for binary classification\n",
    "CLASS_NAMES = {\n",
    "    0: \"True\",\n",
    "    1: \"False Content\"\n",
    "}\n",
    "\n",
    "# Function to analyze class distribution\n",
    "def analyze_class_distribution(labels):\n",
    "    # Count instances of each class\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pie([class_counts.get(i, 0) for i in range(2)], \n",
    "            labels=[f\"{CLASS_NAMES.get(i, f'Class {i}')}\\n({class_counts.get(i, 0)} samples)\" for i in range(2)],\n",
    "            autopct='%1.1f%%')\n",
    "    plt.title('Distribution of Classes in Dataset')\n",
    "    plt.axis('equal')\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    for i in range(2):\n",
    "        print(f\"Class {i} ({CLASS_NAMES.get(i, f'Class {i}')}): {class_counts.get(i, 0)} samples\")\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(predictions, labels):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate macro-averaged metrics (treats all classes equally) \n",
    "    precision_macro = precision_score(labels, predictions, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    # Calculate weighted-averaged metrics (accounts for class imbalance)\n",
    "    precision_weighted = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    precision_per_class = precision_score(labels, predictions, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(labels, predictions, average=None)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"precision_weighted\": precision_weighted,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_per_class\": precision_per_class.tolist(),\n",
    "        \"f1_per_class\": f1_per_class.tolist()\n",
    "    }\n",
    "\n",
    "# Main function with model type parameter\n",
    "def small_model_trainer(model_type=\"tinybert\"):\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Path to the dataset\n",
    "    data_path = r\"C:\\Users\\CoolA\\Code\\extracted_500.txt\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Parse the line\n",
    "            parts = line.split(\"[TEXT]\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            after_text = parts[1]\n",
    "            \n",
    "            text_objects_parts = after_text.split(\"[OBJECTS]\")\n",
    "            if len(text_objects_parts) != 2:\n",
    "                continue\n",
    "            text_part = text_objects_parts[0].strip()\n",
    "            after_objects = text_objects_parts[1]\n",
    "            \n",
    "            objects_label_parts = after_objects.split(\"[LABEL]\")\n",
    "            if len(objects_label_parts) != 2:\n",
    "                continue\n",
    "            objects_part = objects_label_parts[0].strip()\n",
    "            label_part = objects_label_parts[1].strip()\n",
    "            \n",
    "            label_int = int(label_part)\n",
    "            \n",
    "            # Convert multi-class to binary (0 = True, 1-5 = False)\n",
    "            binary_label = 0 if label_int == 0 else 1\n",
    "            \n",
    "            # Convert the objects_part (comma-separated) into a Python list\n",
    "            objects_list = [obj.strip() for obj in objects_part.split(\",\") if obj.strip()]\n",
    "            \n",
    "            # Build a short descriptive sentence for the objects\n",
    "            if len(objects_list) > 0:\n",
    "                object_sentence = f\"The image contains: {', '.join(objects_list)}.\"\n",
    "            else:\n",
    "                object_sentence = \"No objects detected.\"\n",
    "            \n",
    "            # Combine the main text with the object sentence\n",
    "            combined_input = f\"{text_part}. {object_sentence}\"\n",
    "            \n",
    "            texts.append(combined_input)\n",
    "            labels.append(binary_label)\n",
    "\n",
    "    # Split into train and test (70/30)\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Analyze class distribution\n",
    "    print(\"Analyzing class distribution...\")\n",
    "    analyze_class_distribution(labels)\n",
    "\n",
    "    # Load model and tokenizer based on model_type\n",
    "    print(f\"\\nInitializing tokenizer and model for {model_type}...\")\n",
    "    \n",
    "    if model_type.lower() == \"tinybert\":\n",
    "        model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,  # 2-way classification\n",
    "            problem_type=\"single_label_classification\"\n",
    "        )\n",
    "    elif model_type.lower() == \"distilbert\":\n",
    "        model_name = \"distilbert-base-uncased\"\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=2  # 2-way classification\n",
    "        )\n",
    "    elif model_type.lower() == \"roberta\":\n",
    "        model_name = \"FacebookAI/xlm-roberta-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,  # 2-way classification\n",
    "            problem_type=\"single_label_classification\"\n",
    "        )\n",
    "    elif model_type.lower() == \"deberta\":\n",
    "        model_name = \"microsoft/deberta-v3-large\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,  # 2-way classification\n",
    "            problem_type=\"single_label_classification\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}. Choose 'tinybert', 'distilbert', 'roberta', or 'deberta'.\")\n",
    "        \n",
    "    print(f\"Using {model_name}\")\n",
    "\n",
    "    # Tokenize the datasets - adjust max_length based on model\n",
    "    # DeBERTa and RoBERTa can handle longer sequences efficiently\n",
    "    if model_type.lower() in [\"deberta\", \"roberta\"]:\n",
    "        max_length = 256  # Increased for larger models\n",
    "    else:\n",
    "        max_length = 128\n",
    "    \n",
    "    # Tokenize training data\n",
    "    print(f\"Tokenizing training data with max_length={max_length}...\")\n",
    "    train_encodings = tokenizer(\n",
    "        train_texts, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize test data\n",
    "    print(\"Tokenizing test data...\")\n",
    "    test_encodings = tokenizer(\n",
    "        test_texts, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Convert labels to tensors\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    \n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = TensorDataset(\n",
    "        train_encodings['input_ids'],\n",
    "        train_encodings['attention_mask'],\n",
    "        train_labels\n",
    "    )\n",
    "    \n",
    "    test_dataset = TensorDataset(\n",
    "        test_encodings['input_ids'],\n",
    "        test_encodings['attention_mask'],\n",
    "        test_labels\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders - adjust batch size based on model size\n",
    "    if model_type.lower() == \"deberta\":\n",
    "        batch_size = 8  # Smaller batch size for larger models\n",
    "    elif model_type.lower() == \"roberta\":\n",
    "        batch_size = 12  # Medium batch size for medium models\n",
    "    else:\n",
    "        batch_size = 16  # Larger batch size for smaller models\n",
    "    \n",
    "    print(f\"Using batch size of {batch_size} for {model_type}\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Setup GPU/CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # If using multiple GPUs\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Training parameters - adjust based on model size\n",
    "    if model_type.lower() == \"deberta\":\n",
    "        num_epochs = 3  # Fewer epochs for larger models\n",
    "        learning_rate = 2e-5  # Lower learning rate for larger models\n",
    "        weight_decay = 0.01\n",
    "    elif model_type.lower() == \"roberta\":\n",
    "        num_epochs = 4  # Medium number of epochs\n",
    "        learning_rate = 3e-5  # Medium learning rate\n",
    "        weight_decay = 0.01\n",
    "    else:\n",
    "        num_epochs = 5  # More epochs for smaller models\n",
    "        learning_rate = 5e-5  # Higher learning rate for smaller models\n",
    "        weight_decay = 0.01\n",
    "    \n",
    "    print(f\"Training for {num_epochs} epochs with learning rate {learning_rate}\")\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler \n",
    "    # Using OneCycleLR for faster convergence\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=learning_rate,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0.0\n",
    "    #best_model_path = f\"./best_{model_type}_model.pt\"\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for batch in progress_bar:\n",
    "            # Get batch data\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        eval_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Eval]\")\n",
    "            for batch in progress_bar:\n",
    "                # Get batch data\n",
    "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                \n",
    "                # Store predictions and labels\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Accumulate loss\n",
    "                eval_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Calculate average evaluation loss\n",
    "        avg_eval_loss = eval_loss / len(test_loader)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_metrics(all_preds, all_labels)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(f\"Epoch {epoch+1} - Evaluation Results:\")\n",
    "        print(f\"  Loss: {avg_eval_loss:.4f}\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Macro F1: {metrics['f1_macro']:.4f}\")\n",
    "        print(f\"  Weighted F1: {metrics['f1_weighted']:.4f}\")\n",
    "        \n",
    "        # # Save the best model\n",
    "        # if metrics['f1_macro'] > best_f1:\n",
    "        #     best_f1 = metrics['f1_macro']\n",
    "        #     if isinstance(model, nn.DataParallel):\n",
    "        #         torch.save(model.module.state_dict(), best_model_path)\n",
    "        #     else:\n",
    "        #         torch.save(model.state_dict(), best_model_path)\n",
    "        #     print(f\"  New best model saved with F1 Macro: {best_f1:.4f}\")\n",
    "    \n",
    "    # Load the best model for final evaluation\n",
    "    if os.path.exists(best_model_path):\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.load_state_dict(torch.load(best_model_path))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"Loaded best model from {best_model_path}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute and print final metrics\n",
    "    final_metrics = compute_metrics(all_preds, all_labels)\n",
    "    \n",
    "    print(\"\\nFinal Test Set Metrics:\")\n",
    "    print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Macro-averaged Precision: {final_metrics['precision_macro']:.4f}\")\n",
    "    print(f\"Weighted-averaged Precision: {final_metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"Macro-averaged F1: {final_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Weighted-averaged F1: {final_metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class Precision:\")\n",
    "    for i, p in enumerate(final_metrics['precision_per_class']):\n",
    "        if i in CLASS_NAMES:\n",
    "            print(f\"Class {i} ({CLASS_NAMES[i]}): {p:.4f}\")\n",
    "        else:\n",
    "            print(f\"Class {i}: {p:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class F1:\")\n",
    "    for i, f in enumerate(final_metrics['f1_per_class']):\n",
    "        if i in CLASS_NAMES:\n",
    "            print(f\"Class {i} ({CLASS_NAMES[i]}): {f:.4f}\")\n",
    "        else:\n",
    "            print(f\"Class {i}: {f:.4f}\")\n",
    "\n",
    "    # Return metrics for comparison\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Running DeBERTa training:\n",
      "Analyzing class distribution...\n",
      "\n",
      "Class Distribution:\n",
      "Class 0 (True): 219 samples\n",
      "Class 1 (False Content): 281 samples\n",
      "\n",
      "Initializing tokenizer and model for deberta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CoolA\\Code\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using microsoft/deberta-v3-large\n",
      "Tokenizing training data with max_length=256...\n",
      "Tokenizing test data...\n",
      "Using batch size of 8 for deberta\n",
      "Using device: cuda\n",
      "Training for 3 epochs with learning rate 2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]: 100%|██████████| 44/44 [00:13<00:00,  3.19it/s, loss=0.6687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Training Loss: 0.7007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Eval]: 100%|██████████| 19/19 [00:01<00:00, 12.17it/s, loss=0.6913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation Results:\n",
      "  Loss: 0.6812\n",
      "  Accuracy: 0.5600\n",
      "  Macro F1: 0.3590\n",
      "  Weighted F1: 0.4021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [Train]: 100%|██████████| 44/44 [00:13<00:00,  3.28it/s, loss=0.5027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Training Loss: 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 [Eval]: 100%|██████████| 19/19 [00:01<00:00, 12.20it/s, loss=0.7040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation Results:\n",
      "  Loss: 0.6112\n",
      "  Accuracy: 0.6000\n",
      "  Macro F1: 0.4802\n",
      "  Weighted F1: 0.5102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Train]: 100%|██████████| 44/44 [00:13<00:00,  3.34it/s, loss=0.4947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Training Loss: 0.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 [Eval]: 100%|██████████| 19/19 [00:01<00:00, 12.47it/s, loss=0.6623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation Results:\n",
      "  Loss: 0.5421\n",
      "  Accuracy: 0.7467\n",
      "  Macro F1: 0.7228\n",
      "  Weighted F1: 0.7326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 19/19 [00:01<00:00, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Metrics:\n",
      "Accuracy: 0.7467\n",
      "Macro-averaged Precision: 0.7795\n",
      "Weighted-averaged Precision: 0.7711\n",
      "Macro-averaged F1: 0.7228\n",
      "Weighted-averaged F1: 0.7326\n",
      "\n",
      "Per-class Precision:\n",
      "Class 0 (True): 0.8500\n",
      "Class 1 (False Content): 0.7091\n",
      "\n",
      "Per-class F1:\n",
      "Class 0 (True): 0.6415\n",
      "Class 1 (False Content): 0.8041\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Running DeBERTa training:\")\n",
    "small_model_trainer(\"deberta\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Running RoBERTa training:\n",
      "Analyzing class distribution...\n",
      "\n",
      "Class Distribution:\n",
      "Class 0 (True): 219 samples\n",
      "Class 1 (False Content): 281 samples\n",
      "\n",
      "Initializing tokenizer and model for roberta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FacebookAI/xlm-roberta-base\n",
      "Tokenizing training data with max_length=256...\n",
      "Tokenizing test data...\n",
      "Using batch size of 12 for roberta\n",
      "Using device: cuda\n",
      "Training for 4 epochs with learning rate 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 [Train]: 100%|██████████| 30/30 [00:03<00:00,  8.79it/s, loss=0.7512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Training Loss: 0.7225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 [Eval]: 100%|██████████| 13/13 [00:00<00:00, 37.37it/s, loss=0.6944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation Results:\n",
      "  Loss: 0.6941\n",
      "  Accuracy: 0.4400\n",
      "  Macro F1: 0.3056\n",
      "  Weighted F1: 0.2689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 [Train]: 100%|██████████| 30/30 [00:03<00:00,  9.06it/s, loss=0.6108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Training Loss: 0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 [Eval]: 100%|██████████| 13/13 [00:00<00:00, 36.24it/s, loss=0.6554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation Results:\n",
      "  Loss: 0.6811\n",
      "  Accuracy: 0.5600\n",
      "  Macro F1: 0.3590\n",
      "  Weighted F1: 0.4021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 [Train]: 100%|██████████| 30/30 [00:03<00:00,  9.04it/s, loss=0.8294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Training Loss: 0.6971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 [Eval]: 100%|██████████| 13/13 [00:00<00:00, 36.12it/s, loss=0.6959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation Results:\n",
      "  Loss: 0.6858\n",
      "  Accuracy: 0.5600\n",
      "  Macro F1: 0.3590\n",
      "  Weighted F1: 0.4021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 [Train]: 100%|██████████| 30/30 [00:03<00:00,  9.05it/s, loss=0.7972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average Training Loss: 0.6858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 [Eval]: 100%|██████████| 13/13 [00:00<00:00, 36.76it/s, loss=0.6951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Evaluation Results:\n",
      "  Loss: 0.6861\n",
      "  Accuracy: 0.5600\n",
      "  Macro F1: 0.3590\n",
      "  Weighted F1: 0.4021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 13/13 [00:00<00:00, 38.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Metrics:\n",
      "Accuracy: 0.5600\n",
      "Macro-averaged Precision: 0.2800\n",
      "Weighted-averaged Precision: 0.3136\n",
      "Macro-averaged F1: 0.3590\n",
      "Weighted-averaged F1: 0.4021\n",
      "\n",
      "Per-class Precision:\n",
      "Class 0 (True): 0.0000\n",
      "Class 1 (False Content): 0.5600\n",
      "\n",
      "Per-class F1:\n",
      "Class 0 (True): 0.0000\n",
      "Class 1 (False Content): 0.7179\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Running RoBERTa training:\")\n",
    "small_model_trainer(\"roberta\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Class name mapping - updated for binary classification\n",
    "CLASS_NAMES = {\n",
    "    0: \"True\",\n",
    "    1: \"False Content\"\n",
    "}\n",
    "\n",
    "# Function to analyze class distribution\n",
    "def analyze_class_distribution(labels):\n",
    "    # Count instances of each class\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pie([class_counts.get(i, 0) for i in range(2)], \n",
    "            labels=[f\"{CLASS_NAMES.get(i, f'Class {i}')}\\n({class_counts.get(i, 0)} samples)\" for i in range(2)],\n",
    "            autopct='%1.1f%%')\n",
    "    plt.title('Distribution of Classes in Dataset')\n",
    "    plt.axis('equal')\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    for i in range(2):\n",
    "        print(f\"Class {i} ({CLASS_NAMES.get(i, f'Class {i}')}): {class_counts.get(i, 0)} samples\")\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(predictions, labels):\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate macro-averaged metrics (treats all classes equally) \n",
    "    precision_macro = precision_score(labels, predictions, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    # Calculate weighted-averaged metrics (accounts for class imbalance)\n",
    "    precision_weighted = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    precision_per_class = precision_score(labels, predictions, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(labels, predictions, average=None)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"precision_weighted\": precision_weighted,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_per_class\": precision_per_class.tolist(),\n",
    "        \"f1_per_class\": f1_per_class.tolist()\n",
    "    }\n",
    "\n",
    "# Main function with model type parameter\n",
    "def small_model_trainer(model_type=\"tinybert\"):\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Path to the dataset\n",
    "    data_path = r\"C:\\Users\\CoolA\\Code\\extracted_500.txt\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Parse the line\n",
    "            parts = line.split(\"[TEXT]\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            after_text = parts[1]\n",
    "            \n",
    "            text_objects_parts = after_text.split(\"[OBJECTS]\")\n",
    "            if len(text_objects_parts) != 2:\n",
    "                continue\n",
    "            text_part = text_objects_parts[0].strip()\n",
    "            after_objects = text_objects_parts[1]\n",
    "            \n",
    "            objects_label_parts = after_objects.split(\"[LABEL]\")\n",
    "            if len(objects_label_parts) != 2:\n",
    "                continue\n",
    "            objects_part = objects_label_parts[0].strip()\n",
    "            label_part = objects_label_parts[1].strip()\n",
    "            \n",
    "            label_int = int(label_part)\n",
    "            \n",
    "            # Convert multi-class to binary (0 = True, 1-5 = False)\n",
    "            binary_label = 0 if label_int == 0 else 1\n",
    "            \n",
    "            # Convert the objects_part (comma-separated) into a Python list\n",
    "            objects_list = [obj.strip() for obj in objects_part.split(\",\") if obj.strip()]\n",
    "            \n",
    "            # Build a short descriptive sentence for the objects\n",
    "            if len(objects_list) > 0:\n",
    "                object_sentence = f\"The image contains: {', '.join(objects_list)}.\"\n",
    "            else:\n",
    "                object_sentence = \"No objects detected.\"\n",
    "            \n",
    "            # Combine the main text with the object sentence\n",
    "            combined_input = f\"{text_part}. {object_sentence}\"\n",
    "            \n",
    "            texts.append(combined_input)\n",
    "            labels.append(binary_label)\n",
    "\n",
    "    # Split into train and test (70/30)\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "        texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Analyze class distribution\n",
    "    print(\"Analyzing class distribution...\")\n",
    "    analyze_class_distribution(labels)\n",
    "\n",
    "    # Load model and tokenizer based on model_type\n",
    "    print(f\"\\nInitializing tokenizer and model for {model_type}...\")\n",
    "    \n",
    "    if model_type.lower() == \"tinybert\":\n",
    "        model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,  # 2-way classification\n",
    "            problem_type=\"single_label_classification\"\n",
    "        )\n",
    "    elif model_type.lower() == \"distilbert\":\n",
    "        model_name = \"distilbert-base-uncased\"\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=2  # 2-way classification\n",
    "        )\n",
    "    elif model_type.lower() == \"roberta-large\":\n",
    "        model_name = \"FacebookAI/roberta-large\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,  # 2-way classification\n",
    "            problem_type=\"single_label_classification\"\n",
    "        )\n",
    "    elif model_type.lower() == \"roberta-base\":\n",
    "        model_name = \"FacebookAI/roberta-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,  # 2-way classification\n",
    "            problem_type=\"single_label_classification\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}. Choose 'tinybert', 'distilbert', 'roberta-large', or 'roberta-base'.\")\n",
    "        \n",
    "    print(f\"Using {model_name}\")\n",
    "\n",
    "    # Tokenize the datasets - adjust max_length based on model\n",
    "    # DeBERTa and RoBERTa can handle longer sequences efficiently\n",
    "    if model_type.lower() in [\"deberta\", \"roberta\"]:\n",
    "        max_length = 256  # Increased for larger models\n",
    "    else:\n",
    "        max_length = 128\n",
    "    \n",
    "    # Tokenize training data\n",
    "    print(f\"Tokenizing training data with max_length={max_length}...\")\n",
    "    train_encodings = tokenizer(\n",
    "        train_texts, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize test data\n",
    "    print(\"Tokenizing test data...\")\n",
    "    test_encodings = tokenizer(\n",
    "        test_texts, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Convert labels to tensors\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    \n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = TensorDataset(\n",
    "        train_encodings['input_ids'],\n",
    "        train_encodings['attention_mask'],\n",
    "        train_labels\n",
    "    )\n",
    "    \n",
    "    test_dataset = TensorDataset(\n",
    "        test_encodings['input_ids'],\n",
    "        test_encodings['attention_mask'],\n",
    "        test_labels\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders - adjust batch size based on model size\n",
    "    if model_type.lower() == \"deberta\":\n",
    "        batch_size = 8  # Smaller batch size for larger models\n",
    "    elif model_type.lower() == \"roberta\":\n",
    "        batch_size = 12  # Medium batch size for medium models\n",
    "    else:\n",
    "        batch_size = 16  # Larger batch size for smaller models\n",
    "    \n",
    "    print(f\"Using batch size of {batch_size} for {model_type}\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Setup GPU/CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # If using multiple GPUs\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Training parameters - adjust based on model size\n",
    "    if model_type.lower() == \"deberta\":\n",
    "        num_epochs = 3  # Fewer epochs for larger models\n",
    "        learning_rate = 2e-5  # Lower learning rate for larger models\n",
    "        weight_decay = 0.01\n",
    "    elif model_type.lower() == \"roberta-large\":\n",
    "        num_epochs = 4  # Medium number of epochs\n",
    "        learning_rate = 3e-5  # Medium learning rate\n",
    "        weight_decay = 0.01\n",
    "    else:\n",
    "        num_epochs = 5  # More epochs for smaller models\n",
    "        learning_rate = 5e-5  # Higher learning rate for smaller models\n",
    "        weight_decay = 0.01\n",
    "    \n",
    "    print(f\"Training for {num_epochs} epochs with learning rate {learning_rate}\")\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler \n",
    "    # Using OneCycleLR for faster convergence\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=learning_rate,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0.0\n",
    "    best_model_path = f\"./best_{model_type}_model.pt\"\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for batch in progress_bar:\n",
    "            # Get batch data\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        eval_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Eval]\")\n",
    "            for batch in progress_bar:\n",
    "                # Get batch data\n",
    "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Get predictions\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                \n",
    "                # Store predictions and labels\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Accumulate loss\n",
    "                eval_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Calculate average evaluation loss\n",
    "        avg_eval_loss = eval_loss / len(test_loader)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_metrics(all_preds, all_labels)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(f\"Epoch {epoch+1} - Evaluation Results:\")\n",
    "        print(f\"  Loss: {avg_eval_loss:.4f}\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Macro F1: {metrics['f1_macro']:.4f}\")\n",
    "        print(f\"  Weighted F1: {metrics['f1_weighted']:.4f}\")\n",
    "        \n",
    "        # # Save the best model\n",
    "        # if metrics['f1_macro'] > best_f1:\n",
    "        #     best_f1 = metrics['f1_macro']\n",
    "        #     if isinstance(model, nn.DataParallel):\n",
    "        #         torch.save(model.module.state_dict(), best_model_path)\n",
    "        #     else:\n",
    "        #         torch.save(model.state_dict(), best_model_path)\n",
    "        #     print(f\"  New best model saved with F1 Macro: {best_f1:.4f}\")\n",
    "    \n",
    "    # Load the best model for final evaluation\n",
    "    if os.path.exists(best_model_path):\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.load_state_dict(torch.load(best_model_path))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"Loaded best model from {best_model_path}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute and print final metrics\n",
    "    final_metrics = compute_metrics(all_preds, all_labels)\n",
    "    \n",
    "    print(\"\\nFinal Test Set Metrics:\")\n",
    "    print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Macro-averaged Precision: {final_metrics['precision_macro']:.4f}\")\n",
    "    print(f\"Weighted-averaged Precision: {final_metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"Macro-averaged F1: {final_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Weighted-averaged F1: {final_metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class Precision:\")\n",
    "    for i, p in enumerate(final_metrics['precision_per_class']):\n",
    "        if i in CLASS_NAMES:\n",
    "            print(f\"Class {i} ({CLASS_NAMES[i]}): {p:.4f}\")\n",
    "        else:\n",
    "            print(f\"Class {i}: {p:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class F1:\")\n",
    "    for i, f in enumerate(final_metrics['f1_per_class']):\n",
    "        if i in CLASS_NAMES:\n",
    "            print(f\"Class {i} ({CLASS_NAMES[i]}): {f:.4f}\")\n",
    "        else:\n",
    "            print(f\"Class {i}: {f:.4f}\")\n",
    "\n",
    "    # Return metrics for comparison\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Running RoBERTa training:\n",
      "Analyzing class distribution...\n",
      "\n",
      "Class Distribution:\n",
      "Class 0 (True): 219 samples\n",
      "Class 1 (False Content): 281 samples\n",
      "\n",
      "Initializing tokenizer and model for roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FacebookAI/roberta-base\n",
      "Tokenizing training data with max_length=128...\n",
      "Tokenizing test data...\n",
      "Using batch size of 16 for roberta-base\n",
      "Using device: cuda\n",
      "Training for 5 epochs with learning rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 22/22 [00:01<00:00, 11.47it/s, loss=0.6613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Training Loss: 0.7042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 41.03it/s, loss=0.7064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation Results:\n",
      "  Loss: 0.6836\n",
      "  Accuracy: 0.5600\n",
      "  Macro F1: 0.3590\n",
      "  Weighted F1: 0.4021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 22/22 [00:01<00:00, 12.64it/s, loss=0.7094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Training Loss: 0.6820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 40.17it/s, loss=0.7619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation Results:\n",
      "  Loss: 0.6759\n",
      "  Accuracy: 0.5933\n",
      "  Macro F1: 0.4372\n",
      "  Weighted F1: 0.4728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 22/22 [00:01<00:00, 12.40it/s, loss=0.7617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Training Loss: 0.6551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 40.06it/s, loss=0.7287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation Results:\n",
      "  Loss: 0.6499\n",
      "  Accuracy: 0.5667\n",
      "  Macro F1: 0.3754\n",
      "  Weighted F1: 0.4169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 22/22 [00:01<00:00, 12.66it/s, loss=0.3544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average Training Loss: 0.4864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 40.02it/s, loss=0.7456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Evaluation Results:\n",
      "  Loss: 0.5722\n",
      "  Accuracy: 0.7267\n",
      "  Macro F1: 0.7231\n",
      "  Weighted F1: 0.7269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 22/22 [00:01<00:00, 12.70it/s, loss=0.2083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Average Training Loss: 0.3130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 39.98it/s, loss=0.7906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Evaluation Results:\n",
      "  Loss: 0.6016\n",
      "  Accuracy: 0.7333\n",
      "  Macro F1: 0.7294\n",
      "  Weighted F1: 0.7333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 10/10 [00:00<00:00, 42.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Metrics:\n",
      "Accuracy: 0.7333\n",
      "Macro-averaged Precision: 0.7294\n",
      "Weighted-averaged Precision: 0.7333\n",
      "Macro-averaged F1: 0.7294\n",
      "Weighted-averaged F1: 0.7333\n",
      "\n",
      "Per-class Precision:\n",
      "Class 0 (True): 0.6970\n",
      "Class 1 (False Content): 0.7619\n",
      "\n",
      "Per-class F1:\n",
      "Class 0 (True): 0.6970\n",
      "Class 1 (False Content): 0.7619\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Running RoBERTa training:\")\n",
    "small_model_trainer(\"roberta-base\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Running RoBERTa training:\n",
      "Analyzing class distribution...\n",
      "\n",
      "Class Distribution:\n",
      "Class 0 (True): 219 samples\n",
      "Class 1 (False Content): 281 samples\n",
      "\n",
      "Initializing tokenizer and model for roberta-large...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FacebookAI/roberta-large\n",
      "Tokenizing training data with max_length=128...\n",
      "Tokenizing test data...\n",
      "Using batch size of 16 for roberta-large\n",
      "Using device: cuda\n",
      "Training for 4 epochs with learning rate 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 [Train]: 100%|██████████| 22/22 [00:04<00:00,  5.08it/s, loss=0.7509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Training Loss: 0.7336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 20.71it/s, loss=0.7008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Evaluation Results:\n",
      "  Loss: 0.7011\n",
      "  Accuracy: 0.4400\n",
      "  Macro F1: 0.3056\n",
      "  Weighted F1: 0.2689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 [Train]: 100%|██████████| 22/22 [00:04<00:00,  5.37it/s, loss=0.7095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Training Loss: 0.6975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 20.66it/s, loss=0.6986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Evaluation Results:\n",
      "  Loss: 0.6736\n",
      "  Accuracy: 0.6800\n",
      "  Macro F1: 0.6791\n",
      "  Weighted F1: 0.6811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 [Train]: 100%|██████████| 22/22 [00:04<00:00,  5.39it/s, loss=0.5043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Training Loss: 0.5913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 20.70it/s, loss=0.9110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluation Results:\n",
      "  Loss: 0.6307\n",
      "  Accuracy: 0.6667\n",
      "  Macro F1: 0.6579\n",
      "  Weighted F1: 0.6645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 [Train]: 100%|██████████| 22/22 [00:04<00:00,  5.39it/s, loss=0.1319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average Training Loss: 0.2968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 [Eval]: 100%|██████████| 10/10 [00:00<00:00, 20.57it/s, loss=1.1453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Evaluation Results:\n",
      "  Loss: 0.7634\n",
      "  Accuracy: 0.6733\n",
      "  Macro F1: 0.6640\n",
      "  Weighted F1: 0.6707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 10/10 [00:00<00:00, 20.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Set Metrics:\n",
      "Accuracy: 0.6733\n",
      "Macro-averaged Precision: 0.6682\n",
      "Weighted-averaged Precision: 0.6711\n",
      "Macro-averaged F1: 0.6640\n",
      "Weighted-averaged F1: 0.6707\n",
      "\n",
      "Per-class Precision:\n",
      "Class 0 (True): 0.6441\n",
      "Class 1 (False Content): 0.6923\n",
      "\n",
      "Per-class F1:\n",
      "Class 0 (True): 0.6080\n",
      "Class 1 (False Content): 0.7200\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Running RoBERTa training:\")\n",
    "small_model_trainer(\"roberta-large\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\CoolA\\Code\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 435063810\n",
      "Trainable parameters: 435063810\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-large\" \n",
    "model = AutoModelForSequenceClassification.from_pretrained(         \n",
    "    model_name,         \n",
    "    num_labels=2,         \n",
    "    problem_type=\"single_label_classification\"     \n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "| Model              | Training Time | VRAM   | Accuracy | Model Size |\n",
    "|--------------------|--------------|--------|----------|------------|\n",
    "| 2-agent framework | N/A          | N/A     | 72.86%   | N/A        |\n",
    "| 3-agent framework | N/A          | N/A     | 57.40%   | N/A        |\n",
    "| Zero-shot Gemini 1.5 Pro | N/A          | N/A     | 68.79%   | N/A        |\n",
    "| Zero-shot Gemini 2.0 Flash | N/A          | N/A     | 72.69%   | N/A        |\n",
    "| DistilBERT        | 43.5s        | 1.1 GB  | 72.00%   | 66M        |\n",
    "| TinyBERT          | 4.6s         | 0.2 GB  | 65.33%   | 14.5M      |\n",
    "| BERT             | 18.6s        | 2.5 GB  | **76.00%**   | 110M       |\n",
    "| DeBERTa V3 Large | 50.1s        | 15.4 GB | 74.67%   | 435M       |\n",
    "| RoBERTa-Base     | 12.6s        | 4.3 GB  | 73.33%   | 125M       |\n",
    "| RoBERTa-Large    | 21.6s        | 7.9 GB  | 67.33%   | 355M       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
